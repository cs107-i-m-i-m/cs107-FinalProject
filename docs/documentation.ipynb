{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 1. Introduction",
   "metadata": {
    "cell_id": "00001-f9f3e947-1769-4924-9976-b4bc363ba9f7",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We will be buidling a library that performs Automatic Differentiation (AD). Any client can install and use the library for personal or professional use and obtain an estimate of the value of the derivative (or gradient / Jacobian in higher dimension) of the function provided at the data point given as argument.\n\nPerforming fast differentiation and obtaining derivatives is absolutely necessary as it is a skill needed for a lot of real life applications. Indeed, most of systems modeling use differential equations to describe a behaviour and these equations require to take derivatives of sometimes complex function. Also, taking the gradient of a function at a given point and cancel it is the most effective way (at least analytically) to find the extrema of a function. Computing the values of extrema is a key feature to optimize a function and the processes it represents.\n\nObtaining the derivative a function is a skill needed for a lot of real life applications. Indeed a lot of models used in different parts of science use differential equations to describe a behavior. As these equations contain explicitly written derivatives, it is important to know how to take them to solve the equations and have a good description of our system.\n\nEven for problems where no derivative is explicitly written it is useful to know how to use them. For convex optimization problems, the global optimum we are looking for is located at the only point where the gradient of the function is null. For more complex cases, taking the derivative of a quantity is an important step of algorithms like Black Box Variational Inference for Bayesian neural networks or Hamiltonian Monte Carlo to obtain samples from any probability distribution.\n\nWith GrADim, we offer a way to compute effectively the derivative of a function using forward and reverse mode (see more details below). Compared to naive methods that could be used to compute a derivative, GrADim will be more precise as it will compute the exact numeric derivatives and not estimations. Also, it will allow the user to access to the computational graph of the function and to see the derivation process step by step. In that way, they will be able to use a tool which is not a black box and which they can easily understand.",
   "metadata": {
    "cell_id": "00000-8dbc70a0-8928-4fa6-bd60-d2fb67cb7797",
    "id": "8FWeIGkD44FJ",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Background \n\nWe will provide a brief background to motivate our implementation of Automatic Differentiaion.\n\n### 1. Intro to AD\n\nAD is a way to obtain the value of the derivative of a function $f$ at a point $X$. The objective is to obtain a method providing more precise values than the naive estimators using Taylor expansion. Such estimators require fine tuning of parameters in order to give an approximation which is close enough to the truth value but which does not fail because of the floating point approximation.\n\n### 2. Chain Rule\n\nThe Chain Rule is the key element of AD. Indeed we can decompose recursively a function $f$ into elementary components. For example, if we consider the function $f(x, y) = cos(x+y) \\times sin(x-y)$, we can write it $f(x,y) = prod(cos(sum(x,y)), sin(difference(x, y))))$. Although unclear for a human eye, such a function is easier to derive by a machine using the chain rule:\n\n$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial x}$\n\nIn other words, you can compute the derivative of a function with respect to a variable by computing recursively the derivatives of each of the components and the derivative of the main function with respect to its components.\n\n### Evaluation graph\n\nWhen we can write a function as a series of simple components, we can obtain its evaluation graph. Here would be the evaluation graph for the example function provided above. \n\n",
   "metadata": {
    "cell_id": "00002-9ac65347-4b83-4096-a3d9-8a4fa9cb428e",
    "id": "VWzl082g44Fu",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "![alt](evaluation_graph.PNG)",
   "metadata": {
    "cell_id": "00003-4d8a61dd-19db-41a3-9fac-a4cf46d28bea",
    "id": "YwRrrcf9getP",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We also have the following evaluation table\n\n|trace|elem operation|value of the function (as a function of $(x,y)$)|elem derivative|$\\nabla_x$|$\\nabla_y$|\n|--|--|--|--|--|--|\n|$u_{-1}$|$u_{-1}$|$x$|$\\dot{u}_{-1}$|$1$|$0$|\n|$u_0$|$u_0$|$y$|$\\dot{u}_0$|$0$|$1$|\n|$u_1$|$u_{-1} + u_0$|$x+y$|$\\dot{u}_{-1} + \\dot{u}_0$|$1$|$1$|\n|$u_2$|$u_{-1} - u_0$|$x-y$|$\\dot{u}_{-1} - \\dot{u}_0$|$1$|$-1$|\n|$u_3$|$cos(u_1)$|$cos(x+y)$|$-\\dot{u}_1sin(u_1)$|$-sin(x+y)$|$-sin(x+y)$|\n|$u_4$|$sin(u_2)$|$sin(x-y)$|$\\dot{u}_2cos(u_2)$|$cos(x-y)$|$-cos(x-y)$|\n|$u_5$|$u_3u_4$|$cos(x+y)sin(x-y)$|$\\dot{u}_3u_4+u_3\\dot{u}_4$|$-sin(x+y)sin(x-y) + cos(x+y)cos(x-y)$|$-sin(x+y)sin(x-y)-cos(x+y)cos(x-y)$|",
   "metadata": {
    "cell_id": "00004-1319bd86-f994-4cc6-ae4a-89d235b6f2f2",
    "id": "-5T3Iv-ggnaP",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Forward mode and reverse mode \n\nIf $f$ is a function with $m$ inputs and $n$ outputs, forward mode is a way to compute the partial derivatives of $f$ with respect to one of its input. To do that, we start from the roots of the evaluation tree (ie the inputs), compute the partial derivatives of all the inputs with respect to the selected one and go down the tree layer by layer computing the partial derivative of each node with respect to its parents. For example, if $g$ is an elementary function and in the evaluation graph $v_i = g(v_j)+v_k$, the partial derivative of $v_i$ will be $\\dot{v}_i = \\dot{v}_jg'(v_j)+\\dot{v_k}$. If we plug the values of $\\dot{v}_j$ and $\\dot{v}_k$ computed before, we find the value of $\\dot{v}_i$.\n\nThe direction (ie the vector) with respect to which the derivative is computed in the forward mode is called the seed vector. The forward mode AD allows to compute the product between the Jacobian of the function and this seed vector. This computation is a linear complexity of the number of states in the computational graph.\n\n\nThe reverse mode is a way to compute the partial derivatives of an output of $f$ with respect to all the inputs. To do that we start by the leaves of the graph (ie the outputs), compute the partial derivatives with respect to their parents and do the same by going up in the evaluation graph. For example, if we already know the value of the partial derivative $\\frac{\\partial f_i}{\\partial v_j}$ and we know that $v_j = g(v_k)$ where $g$ is an elementary function, we can use the chain rule to write $\\frac{\\partial f_i}{\\partial v_k} = \\frac{\\partial f_i}{\\partial v_j}\\times \\frac{\\partial v_j}{\\partial v_k} = \\frac{\\partial f_i}{v_j}\\times g'(v_k)$. \n",
   "metadata": {
    "cell_id": "00011-e18794e2-f496-4128-b647-3898fc4f3ce0",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 3. How to Use GrADim\n\nWe will briefly demonstrate how to use our package\n\n### Installing and importing the package\n\nA user can install the package using:",
   "metadata": {
    "cell_id": "00006-2e7a0168-6173-4f26-9019-b782fcf1923e",
    "id": "DyYntbxO44F6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-acf3e8bb-92f1-454c-9941-55116d216ce7",
    "id": "G0y0QOqb44F_",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "c0b402f4",
    "deepnote_cell_type": "code"
   },
   "source": ">>> pip install GrADim",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The package can be imported using the following command:",
   "metadata": {
    "cell_id": "00008-94e509c4-fa89-42ae-b006-e62642b93d44",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-09073b80-e39b-4017-af62-5c3a21567149",
    "id": "ovswxvHx44GD",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d32007cc",
    "deepnote_cell_type": "code"
   },
   "source": ">>> import GrADim as ad",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Using the package\n\nAn instance of AD object can be created and used to find the derivative for the required point:",
   "metadata": {
    "cell_id": "00010-eb4a64a1-24ce-49f9-813b-17cfc3fff92a",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-1bec7ac0-d9e4-4da1-a6af-276ecfe01a43",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5bf8bb33",
    "deepnote_cell_type": "code"
   },
   "source": ">>> from GrADim.GrADim import Gradim as ad\n>>> from GrADim.forward_mode import ForwardMode\n\n#Value to eval the derivative at is x0\n>>> x0 = 2\n>>> x = ForwardMode(x0)\n\n>>> def fun(x):\n    return 2*ad.exp(x) + ad.cos(x**2)\n\n>>> y = fun(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", y.derivative)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The multiple dimension is handled using numpy arrays. Your functions should always have one input and one output, but its dimension can be multiple.\n\nNote that to handle multiple outputs, one has to use the decorator multiple_outputs as shown in the examples below.",
   "metadata": {
    "tags": [],
    "cell_id": "00012-90fdc41c-38db-446d-9352-65d8be4cf30a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "# Multiple inputs\n>>> X = ForwardMode(np.array([1, 2, 3]))\n\n>>> def fun(x):\n        return x[0] + x[1]*x[2]\n\n>>> y = fun(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", y.derivative)\n\n\n# Multiple outputs\n>>> x = ForwardMode(np.pi)\n>>> @ForwardMode.multiple_outputs\n    def fun2(x):\n        return ad.cos(x), ad.sin(x)\n\n>>> y = fun2(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", y.derivative)\n\n# Multiple inputs and outputs\n>>> X = ForwardMode(np.array([np.pi, 4]))\n>>> @ForwardMode.multiple_outputs\n    def fun3(x):\n        return x[1] * ad.tan(x[0]), x[1]/x[0]\n\n>>> y = fun3(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", y.derivative)\n",
   "metadata": {
    "tags": [],
    "cell_id": "00012-39da1159-985b-472f-9ca9-f51d9471cfa9",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "To use reverse mode, the syntax is almost the same. The only difference is that this time you do not call the derivative attribute of the output of the function but of the input.",
   "metadata": {
    "tags": [],
    "cell_id": "00014-4739e238-f145-4f09-bac6-2ec19a3e0f87",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": ">>> from GrADim.GrADim import Gradim as ad\n>>> from GrADim.reverse_mode import ReverseMode\n\n#Value to eval the derivative at is x0\n>>> x0 = 2\n>>> x = ReverseMode(x0)\n\n>>> def fun(x):\n    return 2*ad.exp(x) + ad.cos(x**2)\n\n>>> y = fun(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", x.derivative)",
   "metadata": {
    "tags": [],
    "cell_id": "00015-89f774a5-cd2e-4e5c-bac4-8ff6b131d245",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Multiple inputs\n>>> X = ReverseMode(np.array([1, 2, 3]))\n\n>>> def fun(x):\n        return x[0] + x[1]*x[2]\n\n>>> y = fun(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", X.derivative)\n\n\n# Multiple outputs\n>>> x = ReverseMode(np.pi)\n>>> @ReverseMode.multiple_outputs\n    def fun2(x):\n        return ad.cos(x), ad.sin(x)\n\n>>> y = fun2(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", x.derivative)\n\n# Multiple inputs and outputs\n>>> X = ReverseMode(np.array([np.pi, 4]))\n>>> @ReverseMode.multiple_outputs\n    def fun3(x):\n        return x[1] * ad.tan(x[0]), x[1]/x[0]\n\n>>> y = fun3(x)\n>>> print(\"Function Value: \", y.value )\n>>> print(\"Function Derivative: \", X.derivative)\n",
   "metadata": {
    "tags": [],
    "cell_id": "00016-aa547cf4-552c-4099-9538-7872d5f9bbdf",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Software Organization \n\n### 1. Directory Structure\n\nThe package directory would look like the following tree.\n\nFile README.md will contain instructions and provide examples using the package. License folder will contain relevant licensing information. File requirements.txt will contain details for package to be distributed.\n\n\n",
   "metadata": {
    "cell_id": "00009-3e330790-7e3e-4a68-b3b1-3071d62957f4",
    "id": "C89QEv6-44GN",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### to review if this has changed before submission",
   "metadata": {
    "tags": [],
    "cell_id": "00014-ce3d2a7a-e393-49fd-ad1d-ea4964696cae",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00013-68e78a93-9c9f-4c96-a1f8-9f9be35153f2",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "f7fe3a1",
    "deepnote_cell_type": "code"
   },
   "source": "master\n├── LICENSE\n├── pyproject.toml\n├── README.md\n├── setup.py     \n├── docs\n│   ├── milestone1.ipynb\n│   ├── milestone2.ipynb\n│   ├── milestone2_progress.ipynb\n│   └── documentation.ipynb\n├── requirements.txt\n├── GrADim\n│   ├── GrADim.py\n│   └── forward_mode.py\n│   └── reverse_mode.py\n├── Tests\n    ├── test_forward.py\n    └── test_reverse.py",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. The basic modules\n\nGrADim is the main module for the library where callable submodules used for automatic differentation will be stored. \n\n### 3. Test Suite\n\nTest will contain testing submodules for GrADim. We have implemented a variety of basic and complement test cases in test_forward_mode.py and test_reverse_mode.py, which covers all functions from the code files GrADim.py, forward_mode.py, and reverse_mode.py. We will use coverage report to provide an assessment of the code coverage. \n\nTo run the shell script, we can go to the main branch of the cs107-FinalProject folder, and run command \"./run_tests.sh\" on the command line in the same folder. The coverage report will be automatically generated in the 'index.html' webpage to be viewed under the folder 'htmlcov' at the same level as the current directory.\n\nThe tests included in the test file include all the basic operations, including addition, multiplication, subtraction, division, power, etc. For all the basic operations, commutative properties of the operators are tested whenever applicable. We have also included tests for more complicated formulas. \n\nA test that examines an elementary function of the forward mode AD object can be:\n\n",
   "metadata": {
    "cell_id": "00014-c1baf8f1-45f7-4762-87dd-bae81ee2474f",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-b5bcd192-9faa-40b6-84a1-8d1937954fbe",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7dd27e26",
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\n\nfrom GrADim.forward_mode import ForwardMode\nfrom GrADim.GrADim import Gradim\n\nclass TestForwardMode:\n    X = ForwardMode(2)\n\n    def test_neg(self):\n        Y = - self.X\n        assert Y.value == -2\n        assert Y.derivative == -1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Similarly, a test that examines an elementray function of the reverse mode AD object can be:",
   "metadata": {
    "tags": [],
    "cell_id": "00022-491297dd-4081-4a67-a8a6-50fe0332eaa7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\nfrom GrADim.reverse_mode import ReverseMode\nfrom GrADim.GrADim import Gradim\n\nclass TestReverseMode:\n\n    def test_addition_c(self):\n        x = ReverseMode(3)\n        g = x + 2\n        assert (float(g.value) == 5.0) & (float(x.derivative) == 1.0)",
   "metadata": {
    "tags": [],
    "cell_id": "00024-945fca90-51d9-480f-bf30-28335539c23b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "After all the tests for the basic functions, we have a number of tests that examine more complicated functions with a blend of different methods of the AD object. For example, after the initial tests defined above, we have tests like the following for the forward mode:\n",
   "metadata": {
    "tags": [],
    "cell_id": "00016-b735fd9a-7933-41ea-acca-1513c696c8e8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00017-1404733e-d425-4921-9c24-28532799f9cb",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "4f2cf4c3",
    "deepnote_cell_type": "code"
   },
   "source": "    def test_polynomial(self):\n        Y = self.X**3 + self.X - 2\n        assert Y.value == 8\n        assert Y.derivative == 13",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We similarly tested for more complicated functions for the reverse mode:",
   "metadata": {
    "tags": [],
    "cell_id": "00026-6e9359f2-8547-48e3-a038-4838bcecfc58",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "def test_complex_function_with_seed(self):\n        x = ReverseMode(3, derivative=2)\n        g = x * Gradim.sin(x)\n        assert (g.value == 3*np.sin(3)) & (x.derivative == 2 * np.sin(3) + 2 * 3 * np.cos(3))",
   "metadata": {
    "tags": [],
    "cell_id": "00027-bfbb97b2-d14b-48d5-b636-c5108d08ed7e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Apart from the tests for all the basic one-to-one functions where there is only one variable in one input function, we have also covered the cases where there are multiple variables in one input function, one variable in multiple input functions, and multiple variables in multiple input functions. For instance, a test case that tests for multiple variables in one input function for forward mode can be:\n",
   "metadata": {
    "tags": [],
    "cell_id": "00019-f563f8cc-1c99-43ac-9978-ec7ab8fa0fcb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "    multiple_X = ForwardMode(np.array([1, 2, 3]))\n\n    def test_function_multiple_inputs(self):\n            def function_multiple_inputs(x):\n                return Gradim.cos(x[0]) + Gradim.exp(x[2])*x[1]\n            Y = function_multiple_inputs(self.multiple_X)\n            assert Y.value == np.cos(1)+np.exp(3)*2\n            assert (Y.derivative == np.array([-np.sin(1), np.exp(3), 2*np.exp(3)])).all()",
   "metadata": {
    "tags": [],
    "cell_id": "00024-90ab1c5e-00f8-4707-96f4-df9060cee046",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Similarly, we have tested for multiple variables for the revserse mode:",
   "metadata": {
    "tags": [],
    "cell_id": "00030-1d9bda09-1e18-4400-ae5f-0cc832165393",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "def test_multiple_inputs_and_outputs_function(self):\n        x = ReverseMode(np.array([2, 3]))\n        @ReverseMode.multiple_outputs\n        def f(x):\n            return x[0] + x[1], x[0] * x[1]\n        g = f(x)\n        assert ((g.value == np.array([5, 6])).all()) & ((x.derivative == np.array([[1, 1], [3, 2]])).all())",
   "metadata": {
    "tags": [],
    "cell_id": "00031-864281f9-990d-4fd5-8363-024907a53e7b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Our test document has a code coverage of 99% for the auto differentiation package.",
   "metadata": {
    "tags": [],
    "cell_id": "00025-79a54988-ee06-4ef6-be04-fb2e7aad068a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 5. Framework considerations\n\nFramework is not currently considered as current implementation is not as complicated. Should the implementation complexity evolves, we will then consider implementing a framework.",
   "metadata": {
    "tags": [],
    "cell_id": "00016-9813f546-d27f-4e37-9ed3-497ee56ac310",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Implementation\n\nWe will now go into some implementation considerations",
   "metadata": {
    "cell_id": "00011-08d9553d-0b69-4a1e-882c-1ad19869a9fc",
    "id": "NoBDMeMt44GW",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Data Structures\nWe will use floats if the user asks for a single output. If the user asks for a number of outputs for several inputs, we will use arrays. Different classes are defined for different input options as explained below.\n\n### 2. Classes\n\nWe will have one class for the generic type of the automatic differentiation object.\n\nInside this generic class, we have one method for calculating derivative values, including a number of if-else statements:\n- For the function input, we have one if-else block to check whether it contains matrices. If yes, the program will call the method of matrix operations for differentiation. Otherwise, call the usual automatic differentiation method.\n- For the number of input variables, we have one if-else block to check if it is larger than 1. If univariate, the program will implement the differentiation function with only one variable. If multivariate, the program calls the multivariate differentiation method.\n- For univariate differentiation, we have one nested if-else block to check whether the input variable is a single value or an array of values. If input is a single number, the program will implement simple differentiation. Otherwise, the input is an array, then the program will iterate through the array of values for simple differentiation.\n- For multivariate differentiation, we have a nested if-else block to check whether the input variable is a matrix of  values. If it is a vector, the program will implement multivariate automatic differentiation. Otherwise, the input values are in matrix form, then the program will iterate through each vector and call the multivariate automatic differentiation.\n- For the function implemented, we have one if-else block to check if the function contains matrices. If it contains matrices, the program will implement the matrix version of differentiation, in univariate or multivariate form, depending on the number of input variables. Otherwise, the program will implement the usual form of differentiation that do not invlove matrix multiplication.\n\nFor automatic differentiation, an elementary operation would be something like:",
   "metadata": {
    "cell_id": "00012-c3427a3a-9ca0-4550-a89a-38dc1945cf4d",
    "id": "HiJJW5Y-44Ga",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-d965680e-30c5-432a-8d9b-ea6274b8d251",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b3a862f8",
    "deepnote_cell_type": "code"
   },
   "source": "def sin(self, x):\n    if x is a vector:\n        for variable in vector:\n            self.partial_variable = self.cos(x)\n            self.val = self.sin(x)\n\n    else:\n        self.der = self.cos(x)\n        self.val = self.sin(x)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We have one subclass of differentiation that implements the most basic form of differentiation: input has one variable and one function, and the output is one value representing the function derivative. \nWe have one subclass of differentiation, partial differentiation, that handles the input function with more than one variables.\nWe have one subclass of differentiation for automatic differentiation that covers the case where the input function is in matrix form.\n\nMethods that all classes share include hard coding the basic differentiations, (e.g., $\\frac{dc}{dx}$ = 0 where c is a constant, $\\frac{d sin(x)}{dx}$ = cos(x), $\\frac{dx^a}{dx}$ = ax<sup>a-1</sup>etc.) and chain rule. For multivariate differentiation, methods are defined to calculate partial derivatives ($\\frac{\\partial (xy)}{\\partial x} = y, \\frac{\\partial (xy)}{\\partial y} = x$). When necessary, we will overwrite the pre-defined methods so that the program would do the differentiation. \n\nWe will be implementing operator overloading to natural operators such as addition, multiplication, subtraction, and raising to a power etc.\n\nName attributes contain function values and derivatives.\n\n### 3. External dependencies\nWe will use numpy elementary operations (e.g., sin, cos, log, exp, tan, sinh, cosh, tanh, arcsin, arctan, etc.). We will use scipy is used mainly for more complicated matrix algebra. If the input function is passed in via scipy, we may use a scipy dependency in implementation.\n\nWe are using pytest for testing files. \n\n### 4. Other considerations \nWe will consider differentiation in polar coordinate as well as that in cartesian coordinate.",
   "metadata": {
    "cell_id": "00014-b2534af7-3d34-4f57-9653-ad7791d219ed",
    "id": "Og8Q-q7s44Gd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Broader Impact and Inclusivity Statement \nAs we design this software, our intention is to contribute to the community, by providing an easier way to perform differentiation. Despite our well-intention, we are conscious that that could be a gap between the real world impacts of our work, and also the way which the computing community view our work. There are definitely downsides of our work, beyond what the software is intended to do. \n\nWhile it is not our intention, our software inherently discriminates. It assumes that users have a prior knowledge of basic computing, calculus. This discriminates against people who are not literate in computing or not well-versed in calculus. Our content and instructions are in English, which is another form of discrimination against non-English speakers. We recognize that we can address such forms of discriminations by making our software easier to use. This could be the form of developing Graphical-User-Interface so that less computing language is required. We can also develop our software in different languages to cater to people of different backgrounds. Even though these are not addressed yet due to the scale of our project, we recognize the importance of inclusivity. \n\nOur software could also have an impact on the job market, as with every other automation that comes with technology. One may argue that it is to a lesser extent, due to the already prevalent automatic differentiation tools when this software was developed. Nevertheless, the impact should not be ignored. We however design this with the hope that it could be an upskilling resource available to increase the productivity, and employability of workers. \n\nWith the recognition of the downsides that our existing software could bring to the wider community, we are taking notes on how we could possibly improve our software at its next possible development stage. \n",
   "metadata": {
    "tags": [],
    "cell_id": "00025-81e5c363-5d51-465f-a1b9-8b6e369ecb54",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 7. Future Features\n\nWe hope to extend our work to make it more accessible and more intuitive to use. Some of the future extentstions we aim to add are:\n\n1. A GUI that makes the package visually easy to use. \n2. To make it more accesible, inculde text-to-speech facility to help the visually impaired use the packages\n3. Extending the reachable userbase by using text translation library to make the package and its documentation available in multiple languages. \n4. Conduct a survey among the disadvantaged groups and implement improvements that will be most valuable to them based on the feedback\n\nOn the technical side, we would also like to include some additional features:\n\n1. The facility to natively implement and train a deep neural network and use our package to implement backpropagation.\n2. Ability to perform differentiation in polar coordinate and cylindrical as well as that in cartesian coordinate.\n3. Include the functionality do higher order derivatives (i.e. Hessian) which can be used in large scale optimization problems among other applications. \n\nWith the technical improvements, we hope to apply the software in practical industrial problems in areas such as finance or insurance.\n",
   "metadata": {
    "tags": [],
    "cell_id": "00026-8ce66e83-9f61-43e0-a708-2b22fc233d1c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "\n# 8. Licensing \n\nWe would want our library to be open source and accessible to everyone. We would be okay with others mmodifying our code and if the modifications are distributed, and if it is used in comercial softwares, with proper acknowledgement. So we opt for MIT copyright license. ",
   "metadata": {
    "cell_id": "00015-d2251d10-9fbc-4d50-8d92-7dc4ec4f26d0",
    "id": "tQeSkW2U44Gg",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=54bed6c1-8585-41f6-b0fd-156510d7a518' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "milestone1.ipynb",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f0462fec-6caa-4b39-b27e-3e61a9962d74",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 }
}