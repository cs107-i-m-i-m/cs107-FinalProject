{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 1. Introduction\n\nWe will be buidling a library that performs Automatic Differentiation (AD). Any client can install and use the library for personal or professional use and obtain an estimate of the value of the derivative (or gradient / Jacobian in higher dimension) of the function provided at the data point given as argument.\n\nPerforming fast differentiation and obtaining derivatives is absolutely necessary as it is a skill needed for a lot of real life applications. Indeed, most of systems modeling use differential equations to describe a behaviour and these equations require to take derivatives of sometimes complex function. Also, taking the gradient of a function at a given point and cancel it is the most effective way (at least analytically) to find the extrema of a function. Computing the values of extrema is a key feature to optimize a function and the processes it represents.\n\n",
   "metadata": {
    "id": "8FWeIGkD44FJ",
    "cell_id": "00000-8dbc70a0-8928-4fa6-bd60-d2fb67cb7797",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "- ",
   "metadata": {
    "id": "Woe4g0z444Fm",
    "cell_id": "00001-cc864ea8-0435-4675-b0e0-2d5fa480cc73",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Background \n\nWe will provide a brief background to motivate our implementation of Automatic Differentiaion.\n\n### 1. Intro to AD\n\nAD is a way to obtain the value of the derivative of a function $f$ at a point $X$. The objective is to obtain a method providing more precise values than the naive estimators using Taylor expansion. Such estimators require fine tuning of parameters in order to give an approximation which is close enough to the truth value but which does not fail because of the floating point approximation.\n\n### 2. Chain Rule\n\nThe Chain Rule is the key element of AD. Indeed we can decompose recursively a function $f$ into elementary components. For example, if we consider the function $f(x, y) = cos(x+y) \\times sin(x-y)$, we can write it $f(x,y) = prod(cos(sum(x,y)), sin(difference(x, y))))$. Although unclear for a human eye, such a function is easier to derive by a machine using the chain rule:\n\n$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial x}$\n\nIn other words, you can compute the derivative of a function with respect to a variable by computing recursively the derivatives of each of the components and the derivative of the main function with respect to its components.\n\n### Evaluation graph\n\nWhen we can write a function as a series of simple components, we can obtain its evaluation graph. Here would be the evaluation graph for the example function provided above. \n\n",
   "metadata": {
    "id": "VWzl082g44Fu",
    "cell_id": "00002-9ac65347-4b83-4096-a3d9-8a4fa9cb428e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "![alt](evaluation_graph.PNG)",
   "metadata": {
    "id": "YwRrrcf9getP",
    "cell_id": "00003-4d8a61dd-19db-41a3-9fac-a4cf46d28bea",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We also have the following evaluation table\n\n|trace|elem operation|value of the function (as a function of $(x,y)$)|elem derivative|$\\nabla_x$|$\\nabla_y$|\n|--|--|--|--|--|--|\n|$u_{-1}$|$u_{-1}$|$x$|$\\dot{u}_{-1}$|$1$|$0$|\n|$u_0$|$u_0$|$y$|$\\dot{u}_0$|$0$|$1$|\n|$u_1$|$u_{-1} + u_0$|$x+y$|$\\dot{u}_{-1} + \\dot{u}_0$|$1$|$1$|\n|$u_2$|$u_{-1} - u_0$|$x-y$|$\\dot{u}_{-1} - \\dot{u}_0$|$1$|$-1$|\n|$u_3$|$cos(u_1)$|$cos(x+y)$|$-\\dot{u}_1sin(u_1)$|$-sin(x+y)$|$-sin(x+y)$|\n|$u_4$|$sin(u_2)$|$sin(x-y)$|$\\dot{u}_2cos(u_2)$|$cos(x-y)$|$-cos(x-y)$|\n|$u_5$|$u_3u_4$|$cos(x+y)sin(x-y)$|$\\dot{u}_3u_4+u_3\\dot{u}_4$|$-sin(x+y)sin(x-y) + cos(x+y)cos(x-y)$|$-sin(x+y)sin(x-y)-cos(x+y)cos(x-y)$|",
   "metadata": {
    "id": "-5T3Iv-ggnaP",
    "cell_id": "00004-1319bd86-f994-4cc6-ae4a-89d235b6f2f2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "- ",
   "metadata": {
    "id": "aSlx3cSq44F2",
    "cell_id": "00005-d6218ec6-e2d4-4faa-9bbe-93b2be160d74",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 3. How to Use GrADim\n\nWe will briefly demonstrate how to use our package\n\n### Installing and importing the package\n\nA user can install the package using:",
   "metadata": {
    "id": "DyYntbxO44F6",
    "cell_id": "00006-2e7a0168-6173-4f26-9019-b782fcf1923e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G0y0QOqb44F_",
    "cell_id": "00007-acf3e8bb-92f1-454c-9941-55116d216ce7",
    "deepnote_cell_type": "code"
   },
   "source": ">>> pip install GrADim",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The package can be imported using the following command:",
   "metadata": {
    "tags": [],
    "cell_id": "00008-94e509c4-fa89-42ae-b006-e62642b93d44",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ovswxvHx44GD",
    "cell_id": "00008-09073b80-e39b-4017-af62-5c3a21567149",
    "deepnote_cell_type": "code"
   },
   "source": ">>> import GrADim as ad",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Using the package\n\nAn instance of AD object can be created and used to find the derivative for the required point:",
   "metadata": {
    "tags": [],
    "cell_id": "00010-eb4a64a1-24ce-49f9-813b-17cfc3fff92a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-1bec7ac0-d9e4-4da1-a6af-276ecfe01a43",
    "deepnote_cell_type": "code"
   },
   "source": ">>> import GrADim as ad\n>>> import numpy as np\n\n>>> def fun(x,y):\n    return np.cos(x+y)*np.cos(x-y)\n\n>>> autodiff = ad.derivative(fun)\n>>> autodiff.fwd_pass((1,1))\n>>> autodiff.rev_pass()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Software Organization \n\n1. Directory Structure\n    The package directory would look like the following tree.\n\n    File README.md will contain instructions and provide examples using the package. License folder will contain relevant licensing information. File requirements.txt will contain details for package to be distributed.\n\n\n",
   "metadata": {
    "id": "C89QEv6-44GN",
    "cell_id": "00009-3e330790-7e3e-4a68-b3b1-3071d62957f4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00013-68e78a93-9c9f-4c96-a1f8-9f9be35153f2",
    "deepnote_cell_type": "code"
   },
   "source": "master\n├── LICENSE\n├── README.md     \n├── docs\n│   ├── ...\n│   └── ...\n├── requirements.txt\n├── travis.yml\n├── GrADim\n│   ├── ...\n│   └── ...\n├── Tests\n    ├── ...\n    └── ...",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "2. The basic modules\n    GrADim is the main module for the library where callable submodules used for automatic differentation will be stored. Test will contain testing submodules for GrADim.\n\n3. Test Suite\n    We will use both TravisCI and CodeCov. Travis CI will be used to detect when a commit has been made and pushed onto Github, and then try to build the project and run tests each time it happens. Codecov will be used to provide an assessment of the code coverage.\n\n4. Package distribution\n    PyPi will be used to distribute the package based on the format in the directory tree.\n\n5. Framework considerations\n    Framework is not currently considered as current implementation is not as complicated. Should the implementation complexity evolves, we will then consider implementing a framework.",
   "metadata": {
    "tags": [],
    "cell_id": "00014-c1baf8f1-45f7-4762-87dd-bae81ee2474f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Implementation\n\nWe will now go into some implementation considerations",
   "metadata": {
    "id": "NoBDMeMt44GW",
    "cell_id": "00011-08d9553d-0b69-4a1e-882c-1ad19869a9fc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Data Structures\nWe will use floats if the user asks for a single output. If the user asks for a number of outputs for several inputs, we will use arrays. Different classes are defined for different input options as explained below.\n\n### 2. Classes\n\nWe will have one class for the generic type of the automatic differentiation object.\n\nInside this generic class, we have one method for calculating derivative values, including a number of if-else statements:\n- For the function input, we have one if-else block to check whether it contains matrices. If yes, the program will call the method of matrix operations for differentiation. Otherwise, call the usual automatic differentiation method.\n- For the number of input variables, we have one if-else block to check if it is larger than 1. If univariate, the program will implement the differentiation function with only one variable. If multivariate, the program calls the multivariate differentiation method.\n- For univariate differentiation, we have one nested if-else block to check whether the input variable is a single value or an array of values. If input is a single number, the program will implement simple differentiation. Otherwise, the input is an array, then the program will iterate through the array of values for simple differentiation.\n- For multivariate differentiation, we have a nested if-else block to check whether the input variable is a matrix of  values. If it is a vector, the program will implement multivariate automatic differentiation. Otherwise, the input values are in matrix form, then the program will iterate through each vector and call the multivariate automatic differentiation.\n- For the function implemented, we have one if-else block to check if the function contains matrices. If it contains matrices, the program will implement the matrix version of differentiation, in univariate or multivariate form, depending on the number of input variables. Otherwise, the program will implement the usual form of differentiation that do not invlove matrix multiplication.\n\nFor automatic differentiation, an elementary operation would be something like:",
   "metadata": {
    "id": "HiJJW5Y-44Ga",
    "cell_id": "00012-c3427a3a-9ca0-4550-a89a-38dc1945cf4d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00017-d965680e-30c5-432a-8d9b-ea6274b8d251",
    "deepnote_cell_type": "code"
   },
   "source": "def sin(self, x):\n    if x is a vector:\n        for variable in vector:\n            self.partial_variable = self.cos(x)\n            self.val = self.sin(x)\n\n    else:\n        self.der = self.cos(x)\n        self.val = self.sin(x)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We have one subclass of differentiation that implements the most basic form of differentiation: input has one variable and one function, and the output is one value representing the function derivative. \nWe have one subclass of differentiation, partial differentiation, that handles the input function with more than one variables.\nWe have one subclass of differentiation for automatic differentiation that covers the case where the input function is in matrix form.\n\nMethods that all classes share include hard coding the basic differentiations, (e.g., $\\frac{dc}{dx}$ = 0 where c is a constant, $\\frac{d sin(x)}{dx}$ = cos(x), $\\frac{dx^a}{dx}$ = ax<sup>a-1</sup>etc.) and chain rule. For multivariate differentiation, methods are defined to calculate partial derivatives ($\\frac{\\partial (xy)}{\\partial x} = y, \\frac{\\partial (xy)}{\\partial y} = x$). When necessary, we will overwrite the pre-defined methods so that the program would do the differentiation.\n\nName attributes contain function values and derivatives.\n\n### 3. External dependencies\nWe will use numpy elementary operations (e.g., sin, cos, log, exp, tan, sinh, cosh, tanh, arcsin, arctan, etc.). We will use scipy is used mainly for more complicated matrix algebra. If the input function is passed in via scipy, we may use a scipy dependency in implementation.\n\n### 4. Other considerations \nWe will consider differentiation in polar coordinate as well as that in cartesian coordinate.",
   "metadata": {
    "id": "Og8Q-q7s44Gd",
    "cell_id": "00014-b2534af7-3d34-4f57-9653-ad7791d219ed",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Licensing \n\nWe would want our library to be open source and accessible to everyone. We would be okay with others mmodifying our code and if the modifications are distributed, and if it is used in comercial softwares, with proper acknowledgement. So we opt for MIT copyright license. ",
   "metadata": {
    "id": "tQeSkW2U44Gg",
    "cell_id": "00015-d2251d10-9fbc-4d50-8d92-7dc4ec4f26d0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "id": "dqjMlmgD44Gi",
    "cell_id": "00016-84ca3da9-84a4-45bb-ac7d-fcd9898dc7c2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "- ",
   "metadata": {
    "id": "eOz2tQG_44Gj",
    "cell_id": "00017-dc7fcc3f-5231-41c3-ab87-d4b321d9e306",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=077351ea-f043-41bb-a867-a9ce6280de54' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "milestone1.ipynb",
   "provenance": []
  },
  "deepnote_notebook_id": "f0462fec-6caa-4b39-b27e-3e61a9962d74",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}