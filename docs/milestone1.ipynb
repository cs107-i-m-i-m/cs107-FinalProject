{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7d1c64",
   "metadata": {
    "cell_id": "00001-f9f3e947-1769-4924-9976-b4bc363ba9f7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9fff8",
   "metadata": {
    "cell_id": "00000-8dbc70a0-8928-4fa6-bd60-d2fb67cb7797",
    "deepnote_cell_type": "markdown",
    "id": "8FWeIGkD44FJ"
   },
   "source": [
    "We will be buidling a library that performs Automatic Differentiation (AD). Any client can install and use the library for personal or professional use and obtain an estimate of the value of the derivative (or gradient / Jacobian in higher dimension) of the function provided at the data point given as argument.\n",
    "\n",
    "Performing fast differentiation and obtaining derivatives is absolutely necessary as it is a skill needed for a lot of real life applications. Indeed, most of systems modeling use differential equations to describe a behaviour and these equations require to take derivatives of sometimes complex function. Also, taking the gradient of a function at a given point and cancel it is the most effective way (at least analytically) to find the extrema of a function. Computing the values of extrema is a key feature to optimize a function and the processes it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40899c91",
   "metadata": {
    "cell_id": "00001-80c97f43-43cf-403e-b8e5-1254b20071ec",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c826f",
   "metadata": {
    "cell_id": "00002-1cac0a55-30f6-454c-9c79-0966bdebb9b2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "*Would have been nice to see more about why do we care about derivatives anyways and why is GrADim a solution compared to other approaches?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b0a8c",
   "metadata": {
    "cell_id": "00002-76145f41-cbbe-4d56-a483-2b08f7cfcd2f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Obtaining the derivative a function is a skill needed for a lot of real life applications. Indeed a lot of models used in different parts of science use differential equations to describe a behavior. As these equations contain explicitly written derivatives, it is important to know how to take them to solve the equations and have a good description of our system.\n",
    "\n",
    "Even for problems where no derivative is explicitly written it is useful to know how to use them. For convex optimization problems, the global optimum we are looking for is located at the only point where the gradient of the function is null. For more complex cases, taking the derivative of a quantity is an important step of algorithms like Black Box Variational Inference for Bayesian neural networks or Hamiltonian Monte Carlo to obtain samples from any probability distribution.\n",
    "\n",
    "With GrADim, we offer a way to compute effectively the derivative of a function using forward and reverse mode (see more details below). Compared to naive methods that could be used to compute a derivative, GrADim will be more precise as it will compute the exact numeric derivatives and not estimations. Also, it will allow the user to access to the computational graph of the function and to see the derivation process step by step. In that way, they will be able to use a tool which is not a black box and which they can easily understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012771e",
   "metadata": {
    "cell_id": "00002-9ac65347-4b83-4096-a3d9-8a4fa9cb428e",
    "deepnote_cell_type": "markdown",
    "id": "VWzl082g44Fu"
   },
   "source": [
    "# 2. Background \n",
    "\n",
    "We will provide a brief background to motivate our implementation of Automatic Differentiaion.\n",
    "\n",
    "### 1. Intro to AD\n",
    "\n",
    "AD is a way to obtain the value of the derivative of a function $f$ at a point $X$. The objective is to obtain a method providing more precise values than the naive estimators using Taylor expansion. Such estimators require fine tuning of parameters in order to give an approximation which is close enough to the truth value but which does not fail because of the floating point approximation.\n",
    "\n",
    "### 2. Chain Rule\n",
    "\n",
    "The Chain Rule is the key element of AD. Indeed we can decompose recursively a function $f$ into elementary components. For example, if we consider the function $f(x, y) = cos(x+y) \\times sin(x-y)$, we can write it $f(x,y) = prod(cos(sum(x,y)), sin(difference(x, y))))$. Although unclear for a human eye, such a function is easier to derive by a machine using the chain rule:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial x}$\n",
    "\n",
    "In other words, you can compute the derivative of a function with respect to a variable by computing recursively the derivatives of each of the components and the derivative of the main function with respect to its components.\n",
    "\n",
    "### Evaluation graph\n",
    "\n",
    "When we can write a function as a series of simple components, we can obtain its evaluation graph. Here would be the evaluation graph for the example function provided above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6de65",
   "metadata": {
    "cell_id": "00003-4d8a61dd-19db-41a3-9fac-a4cf46d28bea",
    "deepnote_cell_type": "markdown",
    "id": "YwRrrcf9getP"
   },
   "source": [
    "![alt](evaluation_graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0633622",
   "metadata": {
    "cell_id": "00004-1319bd86-f994-4cc6-ae4a-89d235b6f2f2",
    "deepnote_cell_type": "markdown",
    "id": "-5T3Iv-ggnaP"
   },
   "source": [
    "We also have the following evaluation table\n",
    "\n",
    "|trace|elem operation|value of the function (as a function of $(x,y)$)|elem derivative|$\\nabla_x$|$\\nabla_y$|\n",
    "|--|--|--|--|--|--|\n",
    "|$u_{-1}$|$u_{-1}$|$x$|$\\dot{u}_{-1}$|$1$|$0$|\n",
    "|$u_0$|$u_0$|$y$|$\\dot{u}_0$|$0$|$1$|\n",
    "|$u_1$|$u_{-1} + u_0$|$x+y$|$\\dot{u}_{-1} + \\dot{u}_0$|$1$|$1$|\n",
    "|$u_2$|$u_{-1} - u_0$|$x-y$|$\\dot{u}_{-1} - \\dot{u}_0$|$1$|$-1$|\n",
    "|$u_3$|$cos(u_1)$|$cos(x+y)$|$-\\dot{u}_1sin(u_1)$|$-sin(x+y)$|$-sin(x+y)$|\n",
    "|$u_4$|$sin(u_2)$|$sin(x-y)$|$\\dot{u}_2cos(u_2)$|$cos(x-y)$|$-cos(x-y)$|\n",
    "|$u_5$|$u_3u_4$|$cos(x+y)sin(x-y)$|$\\dot{u}_3u_4+u_3\\dot{u}_4$|$-sin(x+y)sin(x-y) + cos(x+y)cos(x-y)$|$-sin(x+y)sin(x-y)-cos(x+y)cos(x-y)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba65ef4",
   "metadata": {
    "cell_id": "00009-8920e047-b06f-434f-a886-9e950c7e6c11",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193caab9",
   "metadata": {
    "cell_id": "00010-a73ae2e0-cc0d-4316-ba7f-5766d4aaf6c2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "*Good start to the background. Going forward, we would like to see more discussion on automatic differentiation. How do forward mode and reverse mode work? We would also like to see more discussion on what forward mode actually computes (Jacobian-vector product), the \"seed\" vector, and the efficiency of forward mode.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60bbbb",
   "metadata": {
    "cell_id": "00011-e18794e2-f496-4128-b647-3898fc4f3ce0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "If $f$ is a function with $m$ inputs and $n$ outputs, forward mode is a way to compute the partial derivatives of $f$ with respect to one of its input. To do that, we start from the roots of the evaluation tree (ie the inputs), compute the partial derivatives of all the inputs with respect to the selected one and go down the tree layer by layer computing the partial derivative of each node with respect to its parents. For example, if $g$ is an elementary function and in the evaluation graph $v_i = g(v_j)+v_k$, the partial derivative of $v_i$ will be $\\dot{v}_i = \\dot{v}_jg'(v_j)+\\dot{v_k}$. If we plug the values of $\\dot{v}_j$ and $\\dot{v}_k$ computed before, we find the value of $\\dot{v}_i$.\n",
    "\n",
    "The direction (ie the vector) with respect to which the derivative is computed in the forward mode is called the seed vector. The forward mode AD allows to compute the product between the Jacobian of the function and this seed vector. This computation is a linear complexity of the number of states in the computational graph.\n",
    "\n",
    "\n",
    "The reverse mode is a way to compute the partial derivatives of an output of $f$ with respect to all the inputs. To do that we start by the leaves of the graph (ie the outputs), compute the partial derivatives with respect to their parents and do the same by going up in the evaluation graph. For example, if we already know the value of the partial derivative $\\frac{\\partial f_i}{\\partial v_j}$ and we know that $v_j = g(v_k)$ where $g$ is an elementary function, we can use the chain rule to write $\\frac{\\partial f_i}{\\partial v_k} = \\frac{\\partial f_i}{\\partial v_j}\\times \\frac{\\partial v_j}{\\partial v_k} = \\frac{\\partial f_i}{v_j}\\times g'(v_k)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75808bc3",
   "metadata": {
    "cell_id": "00006-2e7a0168-6173-4f26-9019-b782fcf1923e",
    "deepnote_cell_type": "markdown",
    "id": "DyYntbxO44F6"
   },
   "source": [
    "# 3. How to Use GrADim\n",
    "\n",
    "We will briefly demonstrate how to use our package\n",
    "\n",
    "### Installing and importing the package\n",
    "\n",
    "A user can install the package using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84194495",
   "metadata": {
    "cell_id": "00007-acf3e8bb-92f1-454c-9941-55116d216ce7",
    "deepnote_cell_type": "code",
    "id": "G0y0QOqb44F_"
   },
   "outputs": [],
   "source": [
    ">>> pip install GrADim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe715e5b",
   "metadata": {
    "cell_id": "00008-94e509c4-fa89-42ae-b006-e62642b93d44",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The package can be imported using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9c646",
   "metadata": {
    "cell_id": "00008-09073b80-e39b-4017-af62-5c3a21567149",
    "deepnote_cell_type": "code",
    "id": "ovswxvHx44GD"
   },
   "outputs": [],
   "source": [
    ">>> import GrADim as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1250550",
   "metadata": {
    "cell_id": "00010-eb4a64a1-24ce-49f9-813b-17cfc3fff92a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Using the package\n",
    "\n",
    "An instance of AD object can be created and used to find the derivative for the required point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be37b5a",
   "metadata": {
    "cell_id": "00011-1bec7ac0-d9e4-4da1-a6af-276ecfe01a43",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    ">>> import GrADim as ad\n",
    ">>> import numpy as np\n",
    "\n",
    ">>> def fun(x,y):\n",
    "    return np.cos(x+y)*np.cos(x-y)\n",
    "\n",
    ">>> autodiff = ad.derivative(fun)\n",
    ">>> autodiff.fwd_pass((1,1))\n",
    ">>> autodiff.rev_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad3a1f",
   "metadata": {
    "cell_id": "00009-3e330790-7e3e-4a68-b3b1-3071d62957f4",
    "deepnote_cell_type": "markdown",
    "id": "C89QEv6-44GN"
   },
   "source": [
    "# 4. Software Organization \n",
    "\n",
    "1. Directory Structure\n",
    "    The package directory would look like the following tree.\n",
    "\n",
    "    File README.md will contain instructions and provide examples using the package. License folder will contain relevant licensing information. File requirements.txt will contain details for package to be distributed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efb6ca",
   "metadata": {
    "cell_id": "00013-68e78a93-9c9f-4c96-a1f8-9f9be35153f2",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "master\n",
    "├── LICENSE\n",
    "├── README.md     \n",
    "├── docs\n",
    "│   ├── ...\n",
    "│   └── ...\n",
    "├── requirements.txt\n",
    "├── travis.yml\n",
    "├── GrADim\n",
    "│   ├── ...\n",
    "│   └── ...\n",
    "├── Tests\n",
    "    ├── ...\n",
    "    └── ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e17e18",
   "metadata": {
    "cell_id": "00014-c1baf8f1-45f7-4762-87dd-bae81ee2474f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "2. The basic modules\n",
    "    GrADim is the main module for the library where callable submodules used for automatic differentation will be stored. Test will contain testing submodules for GrADim.\n",
    "\n",
    "3. Test Suite\n",
    "    We will use both TravisCI and CodeCov. Travis CI will be used to detect when a commit has been made and pushed onto Github, and then try to build the project and run tests each time it happens. Codecov will be used to provide an assessment of the code coverage.\n",
    "\n",
    "4. Package distribution\n",
    "    PyPi will be used to distribute the package based on the format in the directory tree.\n",
    "\n",
    "5. Framework considerations\n",
    "    Framework is not currently considered as current implementation is not as complicated. Should the implementation complexity evolves, we will then consider implementing a framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f81f2",
   "metadata": {
    "cell_id": "00011-08d9553d-0b69-4a1e-882c-1ad19869a9fc",
    "deepnote_cell_type": "markdown",
    "id": "NoBDMeMt44GW"
   },
   "source": [
    "# 5. Implementation\n",
    "\n",
    "We will now go into some implementation considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9b372",
   "metadata": {
    "cell_id": "00012-c3427a3a-9ca0-4550-a89a-38dc1945cf4d",
    "deepnote_cell_type": "markdown",
    "id": "HiJJW5Y-44Ga"
   },
   "source": [
    "### 1. Data Structures\n",
    "We will use floats if the user asks for a single output. If the user asks for a number of outputs for several inputs, we will use arrays. Different classes are defined for different input options as explained below.\n",
    "\n",
    "### 2. Classes\n",
    "\n",
    "We will have one class for the generic type of the automatic differentiation object.\n",
    "\n",
    "Inside this generic class, we have one method for calculating derivative values, including a number of if-else statements:\n",
    "- For the function input, we have one if-else block to check whether it contains matrices. If yes, the program will call the method of matrix operations for differentiation. Otherwise, call the usual automatic differentiation method.\n",
    "- For the number of input variables, we have one if-else block to check if it is larger than 1. If univariate, the program will implement the differentiation function with only one variable. If multivariate, the program calls the multivariate differentiation method.\n",
    "- For univariate differentiation, we have one nested if-else block to check whether the input variable is a single value or an array of values. If input is a single number, the program will implement simple differentiation. Otherwise, the input is an array, then the program will iterate through the array of values for simple differentiation.\n",
    "- For multivariate differentiation, we have a nested if-else block to check whether the input variable is a matrix of  values. If it is a vector, the program will implement multivariate automatic differentiation. Otherwise, the input values are in matrix form, then the program will iterate through each vector and call the multivariate automatic differentiation.\n",
    "- For the function implemented, we have one if-else block to check if the function contains matrices. If it contains matrices, the program will implement the matrix version of differentiation, in univariate or multivariate form, depending on the number of input variables. Otherwise, the program will implement the usual form of differentiation that do not invlove matrix multiplication.\n",
    "\n",
    "For automatic differentiation, an elementary operation would be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4d198",
   "metadata": {
    "cell_id": "00017-d965680e-30c5-432a-8d9b-ea6274b8d251",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sin(self, x):\n",
    "    if x is a vector:\n",
    "        for variable in vector:\n",
    "            self.partial_variable = self.cos(x)\n",
    "            self.val = self.sin(x)\n",
    "\n",
    "    else:\n",
    "        self.der = self.cos(x)\n",
    "        self.val = self.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a85021",
   "metadata": {
    "cell_id": "00014-b2534af7-3d34-4f57-9653-ad7791d219ed",
    "deepnote_cell_type": "markdown",
    "id": "Og8Q-q7s44Gd"
   },
   "source": [
    "We have one subclass of differentiation that implements the most basic form of differentiation: input has one variable and one function, and the output is one value representing the function derivative. \n",
    "We have one subclass of differentiation, partial differentiation, that handles the input function with more than one variables.\n",
    "We have one subclass of differentiation for automatic differentiation that covers the case where the input function is in matrix form.\n",
    "\n",
    "Methods that all classes share include hard coding the basic differentiations, (e.g., $\\frac{dc}{dx}$ = 0 where c is a constant, $\\frac{d sin(x)}{dx}$ = cos(x), $\\frac{dx^a}{dx}$ = ax<sup>a-1</sup>etc.) and chain rule. For multivariate differentiation, methods are defined to calculate partial derivatives ($\\frac{\\partial (xy)}{\\partial x} = y, \\frac{\\partial (xy)}{\\partial y} = x$). When necessary, we will overwrite the pre-defined methods so that the program would do the differentiation.\n",
    "\n",
    "Name attributes contain function values and derivatives.\n",
    "\n",
    "### 3. External dependencies\n",
    "We will use numpy elementary operations (e.g., sin, cos, log, exp, tan, sinh, cosh, tanh, arcsin, arctan, etc.). We will use scipy is used mainly for more complicated matrix algebra. If the input function is passed in via scipy, we may use a scipy dependency in implementation.\n",
    "\n",
    "### 4. Other considerations \n",
    "We will consider differentiation in polar coordinate as well as that in cartesian coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed58a14",
   "metadata": {
    "cell_id": "00025-975c0c90-1d8d-4d5a-a478-a0e6c046d8b9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85cad7",
   "metadata": {
    "cell_id": "00027-d454f033-41cb-4d01-8445-bf1857e9db95",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Will you implement operator overloading methods? If so, what methods will you overload?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b0b2f",
   "metadata": {
    "cell_id": "00027-2b7192ff-47f1-42d2-964c-6731dfcba034",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Yes, we will be implementing operator overloading to natural operators such as addition, multiplication, subtraction, and raising to a power etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c104d4",
   "metadata": {
    "cell_id": "00015-d2251d10-9fbc-4d50-8d92-7dc4ec4f26d0",
    "deepnote_cell_type": "markdown",
    "id": "tQeSkW2U44Gg"
   },
   "source": [
    "# 6. Licensing \n",
    "\n",
    "We would want our library to be open source and accessible to everyone. We would be okay with others mmodifying our code and if the modifications are distributed, and if it is used in comercial softwares, with proper acknowledgement. So we opt for MIT copyright license. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b24aaa",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=077351ea-f043-41bb-a867-a9ce6280de54' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "milestone1.ipynb",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f0462fec-6caa-4b39-b27e-3e61a9962d74",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
